{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7f7cd5cb41f0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/dhruvahuja19/.local/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gzip\n",
    "import json\n",
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse json files and store them in pandas df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../../Data/Video_Games_5.json.gz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m     i \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     11\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mDataFrame\u001b[38;5;241m.\u001b[39mfrom_dict(df, orient\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m purchasing_df \u001b[38;5;241m=\u001b[39m \u001b[43mgetDF\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../../../Data/Video_Games_5.json.gz\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m meta_datadf \u001b[38;5;241m=\u001b[39m getDF(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../../../Data/meta_Video_Games.json.gz\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 8\u001b[0m, in \u001b[0;36mgetDF\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m      6\u001b[0m i \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      7\u001b[0m df \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m parse(path):\n\u001b[1;32m      9\u001b[0m   df[i] \u001b[38;5;241m=\u001b[39m d\n\u001b[1;32m     10\u001b[0m   i \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m, in \u001b[0;36mparse\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse\u001b[39m(path):\n\u001b[0;32m----> 2\u001b[0m   g \u001b[38;5;241m=\u001b[39m \u001b[43mgzip\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m g:\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m json\u001b[38;5;241m.\u001b[39mloads(l)\n",
      "File \u001b[0;32m/usr/lib/python3.10/gzip.py:58\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(filename, mode, compresslevel, encoding, errors, newline)\u001b[0m\n\u001b[1;32m     56\u001b[0m gz_mode \u001b[38;5;241m=\u001b[39m mode\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(filename, (\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mbytes\u001b[39m, os\u001b[38;5;241m.\u001b[39mPathLike)):\n\u001b[0;32m---> 58\u001b[0m     binary_file \u001b[38;5;241m=\u001b[39m \u001b[43mGzipFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgz_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompresslevel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwrite\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     60\u001b[0m     binary_file \u001b[38;5;241m=\u001b[39m GzipFile(\u001b[38;5;28;01mNone\u001b[39;00m, gz_mode, compresslevel, filename)\n",
      "File \u001b[0;32m/usr/lib/python3.10/gzip.py:174\u001b[0m, in \u001b[0;36mGzipFile.__init__\u001b[0;34m(self, filename, mode, compresslevel, fileobj, mtime)\u001b[0m\n\u001b[1;32m    172\u001b[0m     mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fileobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 174\u001b[0m     fileobj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmyfileobj \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    176\u001b[0m     filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(fileobj, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../../Data/Video_Games_5.json.gz'"
     ]
    }
   ],
   "source": [
    "def parse(path):\n",
    "  g = gzip.open(path, 'rb')\n",
    "  for l in g:\n",
    "    yield json.loads(l)\n",
    "def getDF(path):\n",
    "  i = 0\n",
    "  df = {}\n",
    "  for d in parse(path):\n",
    "    df[i] = d\n",
    "    i += 1\n",
    "  return pd.DataFrame.from_dict(df, orient='index')\n",
    "purchasing_df = getDF('../../../Data/Video_Games_5.json.gz')\n",
    "meta_datadf = getDF('../../../Data/meta_Video_Games.json.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping irrelevant columns in positive samples, labeling all purchasing data as a positive sample, and coverting reviewTime to a datatime obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>verified</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>True</td>\n",
       "      <td>2015-10-17</td>\n",
       "      <td>A1HP7NVNPFMA4N</td>\n",
       "      <td>0700026657</td>\n",
       "      <td>Ambrosia075</td>\n",
       "      <td>This game is a bit hard to get the hang of, bu...</td>\n",
       "      <td>but when you do it's great.</td>\n",
       "      <td>1445040000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.0</td>\n",
       "      <td>False</td>\n",
       "      <td>2015-07-27</td>\n",
       "      <td>A1JGAP0185YJI6</td>\n",
       "      <td>0700026657</td>\n",
       "      <td>travis</td>\n",
       "      <td>I played it a while but it was alright. The st...</td>\n",
       "      <td>But in spite of that it was fun, I liked it</td>\n",
       "      <td>1437955200</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>True</td>\n",
       "      <td>2015-02-23</td>\n",
       "      <td>A1YJWEXHQBWK2B</td>\n",
       "      <td>0700026657</td>\n",
       "      <td>Vincent G. Mezera</td>\n",
       "      <td>ok game.</td>\n",
       "      <td>Three Stars</td>\n",
       "      <td>1424649600</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>True</td>\n",
       "      <td>2015-02-20</td>\n",
       "      <td>A2204E1TH211HT</td>\n",
       "      <td>0700026657</td>\n",
       "      <td>Grandma KR</td>\n",
       "      <td>found the game a bit too complicated, not what...</td>\n",
       "      <td>Two Stars</td>\n",
       "      <td>1424390400</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>True</td>\n",
       "      <td>2014-12-25</td>\n",
       "      <td>A2RF5B5H74JLPE</td>\n",
       "      <td>0700026657</td>\n",
       "      <td>jon</td>\n",
       "      <td>great game, I love it and have played it since...</td>\n",
       "      <td>love this game</td>\n",
       "      <td>1419465600</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   overall  verified reviewTime      reviewerID        asin  \\\n",
       "0      5.0      True 2015-10-17  A1HP7NVNPFMA4N  0700026657   \n",
       "1      4.0     False 2015-07-27  A1JGAP0185YJI6  0700026657   \n",
       "2      3.0      True 2015-02-23  A1YJWEXHQBWK2B  0700026657   \n",
       "3      2.0      True 2015-02-20  A2204E1TH211HT  0700026657   \n",
       "4      5.0      True 2014-12-25  A2RF5B5H74JLPE  0700026657   \n",
       "\n",
       "        reviewerName                                         reviewText  \\\n",
       "0        Ambrosia075  This game is a bit hard to get the hang of, bu...   \n",
       "1             travis  I played it a while but it was alright. The st...   \n",
       "2  Vincent G. Mezera                                           ok game.   \n",
       "3         Grandma KR  found the game a bit too complicated, not what...   \n",
       "4                jon  great game, I love it and have played it since...   \n",
       "\n",
       "                                       summary  unixReviewTime  label  \n",
       "0                  but when you do it's great.      1445040000      1  \n",
       "1  But in spite of that it was fun, I liked it      1437955200      1  \n",
       "2                                  Three Stars      1424649600      1  \n",
       "3                                    Two Stars      1424390400      1  \n",
       "4                               love this game      1419465600      1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_to_drop = ['vote', 'style', 'image'] \n",
    "purchasing_df = purchasing_df.drop(columns=columns_to_drop)\n",
    "positive_samples = purchasing_df.copy()\n",
    "positive_samples['label'] = 1 \n",
    "positive_samples['reviewTime'] = pd.to_datetime(positive_samples['reviewTime'])\n",
    "positive_samples.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting the list of also_view items to a set to speed up runtime later. Dropping duplicates in the metadata because otherwise when we inner join with our positive samples, we get a nasty cross product that duplicates some of the positive samples unnecesisarily. Reset the index after dropping the duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>category</th>\n",
       "      <th>tech1</th>\n",
       "      <th>description</th>\n",
       "      <th>fit</th>\n",
       "      <th>title</th>\n",
       "      <th>also_buy</th>\n",
       "      <th>tech2</th>\n",
       "      <th>brand</th>\n",
       "      <th>feature</th>\n",
       "      <th>rank</th>\n",
       "      <th>also_view</th>\n",
       "      <th>main_cat</th>\n",
       "      <th>similar_item</th>\n",
       "      <th>date</th>\n",
       "      <th>price</th>\n",
       "      <th>asin</th>\n",
       "      <th>imageURL</th>\n",
       "      <th>imageURLHighRes</th>\n",
       "      <th>details</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[Video Games, PC, Games]</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>Reversi Sensory Challenger</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>Fidelity Electronics</td>\n",
       "      <td>[]</td>\n",
       "      <td>[&gt;#2,623,937 in Toys &amp;amp; Games (See Top 100 ...</td>\n",
       "      <td>{}</td>\n",
       "      <td>Toys &amp;amp; Games</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0042000742</td>\n",
       "      <td>[https://images-na.ssl-images-amazon.com/image...</td>\n",
       "      <td>[https://images-na.ssl-images-amazon.com/image...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[Video Games, Xbox 360, Games, &lt;/span&gt;&lt;/span&gt;&lt;...</td>\n",
       "      <td></td>\n",
       "      <td>[Brand new sealed!]</td>\n",
       "      <td></td>\n",
       "      <td>Medal of Honor: Warfighter - Includes Battlefi...</td>\n",
       "      <td>[B00PADROYW]</td>\n",
       "      <td></td>\n",
       "      <td>by\\n    \\n    EA Games</td>\n",
       "      <td>[]</td>\n",
       "      <td>[&gt;#67,231 in Video Games (See Top 100 in Video...</td>\n",
       "      <td>{B072NQJCW5, B0050SY5BM, B000TI836G, B000TG530...</td>\n",
       "      <td>Video Games</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t&lt;span class=\"vertica...</td>\n",
       "      <td>0078764343</td>\n",
       "      <td>[https://images-na.ssl-images-amazon.com/image...</td>\n",
       "      <td>[https://images-na.ssl-images-amazon.com/image...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[Video Games, Retro Gaming &amp; Microconsoles, Su...</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>street fighter 2 II turbo super nintendo snes ...</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>Nintendo</td>\n",
       "      <td>[]</td>\n",
       "      <td>[&gt;#134,433 in Video Games (See Top 100 in Vide...</td>\n",
       "      <td>{}</td>\n",
       "      <td>Video Games</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>$0.72</td>\n",
       "      <td>0276425316</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[Video Games, Xbox 360, Accessories, Controlle...</td>\n",
       "      <td></td>\n",
       "      <td>[MAS's Pro Xbox 360 Stick (Perfect 360 Stick) ...</td>\n",
       "      <td></td>\n",
       "      <td>Xbox 360 MAS STICK</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>by\\n    \\n    MAS SYSTEMS</td>\n",
       "      <td>[Original PCB used from Xbox 360 Control Pad (...</td>\n",
       "      <td>[&gt;#105,263 in Video Games (See Top 100 in Vide...</td>\n",
       "      <td>{}</td>\n",
       "      <td>Video Games</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0324411812</td>\n",
       "      <td>[https://images-na.ssl-images-amazon.com/image...</td>\n",
       "      <td>[https://images-na.ssl-images-amazon.com/image...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[Video Games, PC, Games, &lt;/span&gt;&lt;/span&gt;&lt;/span&gt;...</td>\n",
       "      <td></td>\n",
       "      <td>[Phonics Alive! 3, The Speller teaches student...</td>\n",
       "      <td></td>\n",
       "      <td>Phonics Alive! 3: The Speller</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>by\\n    \\n    Advanced Software Pty. Ltd.</td>\n",
       "      <td>[Grades 2-12, Spelling Program, Teaches Spelli...</td>\n",
       "      <td>[&gt;#92,397 in Video Games (See Top 100 in Video...</td>\n",
       "      <td>{B000BCZ7U0}</td>\n",
       "      <td>Video Games</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0439335310</td>\n",
       "      <td>[https://images-na.ssl-images-amazon.com/image...</td>\n",
       "      <td>[https://images-na.ssl-images-amazon.com/image...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                           category tech1  \\\n",
       "0      0                           [Video Games, PC, Games]         \n",
       "1      1  [Video Games, Xbox 360, Games, </span></span><...         \n",
       "2      2  [Video Games, Retro Gaming & Microconsoles, Su...         \n",
       "3      3  [Video Games, Xbox 360, Accessories, Controlle...         \n",
       "4      4  [Video Games, PC, Games, </span></span></span>...         \n",
       "\n",
       "                                         description fit  \\\n",
       "0                                                 []       \n",
       "1                                [Brand new sealed!]       \n",
       "2                                                 []       \n",
       "3  [MAS's Pro Xbox 360 Stick (Perfect 360 Stick) ...       \n",
       "4  [Phonics Alive! 3, The Speller teaches student...       \n",
       "\n",
       "                                               title      also_buy tech2  \\\n",
       "0                         Reversi Sensory Challenger            []         \n",
       "1  Medal of Honor: Warfighter - Includes Battlefi...  [B00PADROYW]         \n",
       "2  street fighter 2 II turbo super nintendo snes ...            []         \n",
       "3                                 Xbox 360 MAS STICK            []         \n",
       "4                      Phonics Alive! 3: The Speller            []         \n",
       "\n",
       "                                       brand  \\\n",
       "0                       Fidelity Electronics   \n",
       "1                     by\\n    \\n    EA Games   \n",
       "2                                   Nintendo   \n",
       "3                  by\\n    \\n    MAS SYSTEMS   \n",
       "4  by\\n    \\n    Advanced Software Pty. Ltd.   \n",
       "\n",
       "                                             feature  \\\n",
       "0                                                 []   \n",
       "1                                                 []   \n",
       "2                                                 []   \n",
       "3  [Original PCB used from Xbox 360 Control Pad (...   \n",
       "4  [Grades 2-12, Spelling Program, Teaches Spelli...   \n",
       "\n",
       "                                                rank  \\\n",
       "0  [>#2,623,937 in Toys &amp; Games (See Top 100 ...   \n",
       "1  [>#67,231 in Video Games (See Top 100 in Video...   \n",
       "2  [>#134,433 in Video Games (See Top 100 in Vide...   \n",
       "3  [>#105,263 in Video Games (See Top 100 in Vide...   \n",
       "4  [>#92,397 in Video Games (See Top 100 in Video...   \n",
       "\n",
       "                                           also_view          main_cat  \\\n",
       "0                                                 {}  Toys &amp; Games   \n",
       "1  {B072NQJCW5, B0050SY5BM, B000TI836G, B000TG530...       Video Games   \n",
       "2                                                 {}       Video Games   \n",
       "3                                                 {}       Video Games   \n",
       "4                                       {B000BCZ7U0}       Video Games   \n",
       "\n",
       "  similar_item date                                              price  \\\n",
       "0                                                                        \n",
       "1                    \\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t<span class=\"vertica...   \n",
       "2                                                                $0.72   \n",
       "3                                                                        \n",
       "4                                                                        \n",
       "\n",
       "         asin                                           imageURL  \\\n",
       "0  0042000742  [https://images-na.ssl-images-amazon.com/image...   \n",
       "1  0078764343  [https://images-na.ssl-images-amazon.com/image...   \n",
       "2  0276425316                                                 []   \n",
       "3  0324411812  [https://images-na.ssl-images-amazon.com/image...   \n",
       "4  0439335310  [https://images-na.ssl-images-amazon.com/image...   \n",
       "\n",
       "                                     imageURLHighRes details  \n",
       "0  [https://images-na.ssl-images-amazon.com/image...     NaN  \n",
       "1  [https://images-na.ssl-images-amazon.com/image...     NaN  \n",
       "2                                                 []     NaN  \n",
       "3  [https://images-na.ssl-images-amazon.com/image...     NaN  \n",
       "4  [https://images-na.ssl-images-amazon.com/image...     NaN  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process_asin_list(asin_list):\n",
    "    return set(asin_list)\n",
    "\n",
    "meta_datadf['also_view'] = meta_datadf['also_view'].apply(process_asin_list)\n",
    "meta_datadf = meta_datadf.reset_index()\n",
    "meta_datadf = meta_datadf.drop_duplicates(subset='asin', keep='first')\n",
    "meta_datadf.reset_index(drop=True, inplace=True)\n",
    "meta_datadf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging the metadata and the positive samples, giving us access to the also_view category. Now we have [asin, price, also_view]. Convert string prices to floats, otherwise place an empty string. We dont drop the NANs, as we fill them in later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "428894"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_samples = positive_samples.merge(meta_datadf, on='asin', how='inner')\n",
    "positive_samples['price'] = pd.to_numeric(positive_samples['price'].str.replace(r'[^\\d.]', '', regex=True), errors='coerce')\n",
    "positive_samples['price'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_to_instances = {} \n",
    "for index, row in positive_samples.iterrows(): \n",
    "    if users_to_instances.get(row['reviewerID']) is None:\n",
    "        users_to_instances[row['reviewerID']] = 0\n",
    "    users_to_instances[row['reviewerID']] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dataset containing only rows where users_to_instance[row[reviewerID]] >= 10 \n",
    "positive_samples = positive_samples[positive_samples['reviewerID'].map(users_to_instances) >= 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_to_instances = {} \n",
    "for index, row in positive_samples.iterrows(): \n",
    "    if item_to_instances.get(row['asin']) is None:\n",
    "        item_to_instances[row['asin']] = 0\n",
    "    item_to_instances[row['asin']] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_samples = positive_samples[positive_samples['asin'].map(item_to_instances) >= 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(186198, 29)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_samples.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More irrelevant columns; can probably merged with above cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['imageURL', 'imageURLHighRes', 'reviewerName', 'reviewText', \n",
    "                   'unixReviewTime', 'date']\n",
    "positive_samples = positive_samples.drop(columns=columns_to_drop, axis=1)\n",
    "positive_samples = positive_samples.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_samples = pd.DataFrame(columns=positive_samples.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load MongoDB data,project out the asin, and corresponding dictionary that maps times to prices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient('mongodb://localhost:27017/')\n",
    "db = client['video_games_db']\n",
    "price_collection = db['keepa']\n",
    "query = {\"df_NEW\": {\"$exists\": True}}\n",
    "document_count = price_collection.count_documents(query)\n",
    "projection = {\"df_NEW\": 1, \"asin\": 1, \"_id\": 0}\n",
    "documents = price_collection.find(query, projection)\n",
    "price_df = pd.DataFrame(documents) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid out of bounds erorrs: If there exists an asin in the purchasing data that doesnt exist in the metadata, then we don't want to search the metadata for it. In hindsight, this might be unncessary given that the inner join probably would have deleted the row from the corresponding table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items_considered = set() \n",
    "for index, row in positive_samples.iterrows(): \n",
    "    items_considered.add(row[\"asin\"])\n",
    "items_considered - set(meta_datadf[\"asin\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next three cells are just for runtime purposes; We treat asin as a primary key. We don't want to have to have to exhaustive search df[\"asin\" == stuff] to find a row. We essentially treat asin as a primary key, although there may be duplicate keys, however we just pick the last one WLOG as we replace the time-variant/person-variant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "asin_to_price = {}\n",
    "for index, row in price_df.iterrows():\n",
    "    asin_to_price[row[\"asin\"]] = row[\"df_NEW\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "asin_to_positive = {} \n",
    "for index, row in positive_samples.iterrows(): \n",
    "    asin_to_positive[row['asin']] = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "asin_to_meta = {}\n",
    "for index, row in meta_datadf.iterrows(): \n",
    "    asin_to_meta[row[\"asin\"]] = index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creating negative samples. The price doesn't matter for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows: 100%|██████████| 186198/186198 [03:09<00:00, 983.92it/s] \n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "dfs_to_concat = [] \n",
    "for index, row in tqdm(positive_samples.iterrows(), total=positive_samples.shape[0], desc=\"Processing rows\"):\n",
    "    current_reviewer = row['reviewerID'] \n",
    "    also_view_items = meta_datadf.iloc[asin_to_meta[row['asin']]]['also_view']\n",
    "    for item in also_view_items: \n",
    "        if item in items_considered: \n",
    "            current_asin = item \n",
    "            current_date = pd.Timestamp(row['reviewTime'])\n",
    "            new_row = positive_samples.iloc[asin_to_positive[current_asin]].to_dict()\n",
    "            new_row['label'] = 0\n",
    "            new_row['reviewTime'] = current_date\n",
    "            new_row[\"reviewerID\"] = current_reviewer\n",
    "            dfs_to_concat.append(new_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4150451, 23)\n",
      "(186198, 23)\n"
     ]
    }
   ],
   "source": [
    "negative_samples = pd.DataFrame(dfs_to_concat)\n",
    "print(negative_samples.shape)\n",
    "print(positive_samples.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_samples = pd.concat([positive_samples, negative_samples], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   overall  verified reviewTime      reviewerID        asin  \\\n",
      "0      5.0      True 2011-08-14   AN3YYDZAS3O1Y  0700099867   \n",
      "1      5.0     False 2011-06-18  A15PIAQT55GNCA  0700099867   \n",
      "2      4.0      True 2011-06-14  A361M14PU2GUEG  0700099867   \n",
      "3      5.0     False 2011-06-13  A2LQCBLLJVVR5T  0700099867   \n",
      "4      1.0      True 2014-06-30  A248LSBZT4P38V  0700099867   \n",
      "\n",
      "                                             summary  label  index  \\\n",
      "0        A step up from Dirt 2 and that is terrific!      1     41   \n",
      "1                      this games is amazing!!!!!!!!      1     41   \n",
      "2                                             DIRT 3      1     41   \n",
      "3              BEST GRAPHICS OF ANY GAME SO FAR !!!!      1     41   \n",
      "4  It might have been a good game, but I never fo...      1     41   \n",
      "\n",
      "                   category tech1  ... also_buy tech2   brand  \\\n",
      "0  [Video Games, PC, Games]        ...       []        Dirt 3   \n",
      "1  [Video Games, PC, Games]        ...       []        Dirt 3   \n",
      "2  [Video Games, PC, Games]        ...       []        Dirt 3   \n",
      "3  [Video Games, PC, Games]        ...       []        Dirt 3   \n",
      "4  [Video Games, PC, Games]        ...       []        Dirt 3   \n",
      "\n",
      "                                             feature  \\\n",
      "0  [DiRT 3 delivers mud, sweat and gears the worl...   \n",
      "1  [DiRT 3 delivers mud, sweat and gears the worl...   \n",
      "2  [DiRT 3 delivers mud, sweat and gears the worl...   \n",
      "3  [DiRT 3 delivers mud, sweat and gears the worl...   \n",
      "4  [DiRT 3 delivers mud, sweat and gears the worl...   \n",
      "\n",
      "                                                rank also_view     main_cat  \\\n",
      "0  [>#50,093 in Video Games (See Top 100 in Video...        {}  Video Games   \n",
      "1  [>#50,093 in Video Games (See Top 100 in Video...        {}  Video Games   \n",
      "2  [>#50,093 in Video Games (See Top 100 in Video...        {}  Video Games   \n",
      "3  [>#50,093 in Video Games (See Top 100 in Video...        {}  Video Games   \n",
      "4  [>#50,093 in Video Games (See Top 100 in Video...        {}  Video Games   \n",
      "\n",
      "  similar_item price details  \n",
      "0                NaN     NaN  \n",
      "1                NaN     NaN  \n",
      "2                NaN     NaN  \n",
      "3                NaN     NaN  \n",
      "4                NaN     NaN  \n",
      "\n",
      "[5 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "print(combined_samples.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_search_closest(arr, target, compare):\n",
    "    low, high = 0, len(arr) - 1\n",
    "    closest = None\n",
    "    closest_diff = float('inf')\n",
    "    while low <= high:\n",
    "        mid = (low + high) // 2\n",
    "        result = compare(arr[mid], target)\n",
    "        if result == 0:\n",
    "            return arr[mid]\n",
    "        elif result < 0:\n",
    "            low = mid + 1\n",
    "        else:\n",
    "            high = mid - 1\n",
    "        diff = abs(result)\n",
    "        if diff < closest_diff:\n",
    "            closest_diff = diff\n",
    "            closest = arr[mid]\n",
    "    return closest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update price information using MongoDB data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows: 100%|██████████| 4336649/4336649 [1:36:54<00:00, 745.85it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "261748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "removed_rows = set()\n",
    "for index, row in tqdm(combined_samples.iterrows(), total=combined_samples.shape[0], desc=\"Processing rows\"):\n",
    "    asin = row[\"asin\"]\n",
    "    price = row[\"price\"]\n",
    "    current_date = row[\"reviewTime\"]\n",
    "    if asin in asin_to_price:\n",
    "        price_list = list(asin_to_price[asin].keys())\n",
    "        closest_date = binary_search_closest(price_list, current_date, lambda x, y: (pd.to_datetime(x) - y).total_seconds())\n",
    "        closest_price = asin_to_price[asin][closest_date]\n",
    "        combined_samples.at[index, \"price\"] = closest_price\n",
    "        # if np.isnan(closest_price):\n",
    "        #         print(\"here\")\n",
    "    else:\n",
    "        removed_rows.add(index)\n",
    "print(len(removed_rows))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove rows not present in the mongoDB data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_samples = combined_samples.drop(removed_rows)\n",
    "combined_samples.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove rows that have nan prices in the mongoDB data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_samples = combined_samples.dropna(subset=['price'])\n",
    "combined_samples.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shuffle combined_samples \n",
    "combined_samples = combined_samples.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label encode categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3609211\n"
     ]
    }
   ],
   "source": [
    "print(len(combined_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_encoder = LabelEncoder()\n",
    "item_encoder = LabelEncoder()\n",
    "brand_encoder = LabelEncoder()\n",
    "combined_samples = combined_samples[['user_id_encoded', 'item_id_encoded', 'brand_id_encoded', 'price', 'description', 'label']]\n",
    "num_users =  combined_samples[\"user_id_encoded\"].nunique()\n",
    "num_items = combined_samples[\"item_id_encoded\"].nunique()\n",
    "num_brands =  combined_samples[\"brand_id_encoded\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = combined_samples[['user_id_encoded', 'item_id_encoded', 'brand_id_encoded', 'price', 'description']].values\n",
    "y = combined_samples['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "positive_indices = np.where(y_train == 1)[0]\n",
    "X_train_positive = X_train[positive_indices]\n",
    "y_train_positive = y_train[positive_indices]\n",
    "positive_indices_test = np.where(y_test == 1)[0]\n",
    "X_test_positive = X_test[positive_indices_test]\n",
    "y_test_positive = y_test[positive_indices_test]\n",
    "for i in range(20):\n",
    "    X_train = np.concatenate((X_train, X_train_positive))\n",
    "    y_train = np.concatenate((y_train, y_train_positive))\n",
    "    X_test = np.concatenate((X_test, X_test_positive))\n",
    "    y_test = np.concatenate((y_test, y_test_positive))\n",
    "\n",
    "permutation = np.random.permutation(len(X_train))\n",
    "X_train_shuffled = X_train[permutation]\n",
    "y_train_shuffled = y_train[permutation]\n",
    "\n",
    "X_train_user = X_train[:, 0].astype(np.int32)\n",
    "X_train_item = X_train[:,1].astype(np.int32)\n",
    "X_train_brand = X_train[:,2].astype(np.int32)\n",
    "X_train_price = X_train[:,3].astype(np.float32)\n",
    "X_train_description = X_train[:, 4].astype(np.str_)\n",
    "\n",
    "X_test_user = X_test[:, 0].astype(np.int32)\n",
    "X_test_item = X_test[:, 1].astype(np.int32)\n",
    "X_test_brand = X_test[:, 2].astype(np.int32)\n",
    "X_test_price = X_test[:, 3].astype(np.float32)\n",
    "X_test_description = X_test[:, 4].astype(np.str_)\n",
    "\n",
    "y_train = y_train.astype(np.float32)\n",
    "y_test = y_test.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "def yield_tokens(data_iter):\n",
    "    for text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "vocab = build_vocab_from_iterator(yield_tokens(X_train_description), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "def text_pipeline(text):\n",
    "    return [vocab[token] for token in tokenizer(text)]\n",
    "\n",
    "X_train_description_processed = [text_pipeline(text) for text in X_train_description]\n",
    "X_test_description_processed = [text_pipeline(text) for text in X_test_description]\n",
    "max_length = max(len(desc) for desc in X_train_description_processed + X_test_description_processed)\n",
    "\n",
    "def pad_sequence(seq):\n",
    "    return seq + [vocab[\"<unk>\"]] * (max_length - len(seq))\n",
    "\n",
    "X_train_description_padded = torch.tensor([pad_sequence(desc) for desc in X_train_description_processed], dtype=torch.long)\n",
    "X_test_description_padded = torch.tensor([pad_sequence(desc) for desc in X_test_description_processed], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_user = torch.tensor(X_train_user, dtype=torch.int32)\n",
    "X_train_item = torch.tensor(X_train_item, dtype=torch.int32)\n",
    "X_train_brand = torch.tensor(X_train_brand, dtype=torch.int32)\n",
    "X_train_price = torch.tensor(X_train_price, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "\n",
    "X_test_user = torch.tensor(X_test_user, dtype=torch.int32)\n",
    "X_test_item = torch.tensor(X_test_item, dtype=torch.int32)\n",
    "X_test_brand = torch.tensor(X_test_brand, dtype=torch.int32)\n",
    "X_test_price = torch.tensor(X_test_price, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_user, X_train_item, X_train_brand, X_train_price, X_train_description_padded, y_train)\n",
    "test_dataset = TensorDataset(X_test_user, X_test_item, X_test_brand, X_test_price, X_test_description_padded, y_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False)\n",
    "\n",
    "class RecommendationModel(nn.Module):\n",
    "    def __init__(self, num_users, num_items, num_brands, vocab_size, embedding_size, description_embedding_size):\n",
    "        super(RecommendationModel, self).__init__()\n",
    "        self.user_embedding = nn.Embedding(num_users, embedding_size)\n",
    "        self.item_embedding = nn.Embedding(num_items, embedding_size)\n",
    "        self.brand_embedding = nn.Embedding(num_brands, embedding_size)\n",
    "        self.description_embedding = nn.Embedding(vocab_size, description_embedding_size)\n",
    "        \n",
    "        self.fc1 = nn.Linear(embedding_size * 3 + 1 + description_embedding_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, user, item, brand, price, description):\n",
    "        user_vec = self.user_embedding(user)\n",
    "        item_vec = self.item_embedding(item)\n",
    "        brand_vec = self.brand_embedding(brand)\n",
    "        desc_vec = self.description_embedding(description).mean(dim=1)  # Average pooling\n",
    "        \n",
    "        x = torch.cat([user_vec, item_vec, brand_vec, price.unsqueeze(1), desc_vec], dim=1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, p=0.5)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.dropout(x, p=0.5)\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "model = RecommendationModel(num_users=num_users, num_items=num_items, num_brands=num_brands, vocab_size=vocab_size, embedding_size=32, description_embedding_size=100)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(outputs, labels):\n",
    "    predictions = (torch.sigmoid(outputs) > 0.5).float()\n",
    "    correct = (predictions == labels).float().sum()\n",
    "    return correct / len(labels)\n",
    "\n",
    "for epoch in range(50):\n",
    "    model.train()\n",
    "    all_outputs = []\n",
    "    all_labels = []\n",
    "    total_loss = 0\n",
    "    total_accuracy = 0\n",
    "    num_batches = 0\n",
    "    for users, items, brands, prices, descriptions, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(users, items, brands, prices, descriptions)\n",
    "        loss = criterion(outputs.squeeze(), labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        total_accuracy += calculate_accuracy(outputs.squeeze(), labels)\n",
    "        num_batches += 1\n",
    "        all_outputs.extend(outputs.detach().cpu().numpy())\n",
    "        all_labels.extend(labels.detach().cpu().numpy())\n",
    "\n",
    "    train_auc = roc_auc_score(all_labels, all_outputs)\n",
    "    train_loss = total_loss / num_batches\n",
    "    train_accuracy = total_accuracy / num_batches\n",
    "    \n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    total_val_accuracy = 0\n",
    "    num_val_batches = 0\n",
    "    all_val_outputs = []\n",
    "    all_val_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for users, items, brands, prices, descriptions, labels in test_loader:\n",
    "            outputs = model(users, items, brands, prices, descriptions)\n",
    "            val_loss = criterion(outputs.squeeze(), labels)\n",
    "            total_val_loss += val_loss.item()\n",
    "            total_val_accuracy += calculate_accuracy(outputs.squeeze(), labels)\n",
    "            num_val_batches += 1\n",
    "            all_val_outputs.extend(outputs.detach().cpu().numpy())\n",
    "            all_val_labels.extend(labels.detach().cpu().numpy())\n",
    "        \n",
    "    val_loss = total_val_loss / num_val_batches\n",
    "    val_accuracy = total_val_accuracy / num_val_batches\n",
    "    model.train()\n",
    "    val_auc = roc_auc_score(all_val_labels, all_val_outputs)\n",
    "    print(f\"Epoch {epoch+1}/{50}\")\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}\")\n",
    "    print(f\"Train AUC: {train_auc:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
    "    print(f\"Val AUC: {val_auc:.4f}\")\n",
    "    print(\"--------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
