{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gzip\n",
    "import json\n",
    "from pymongo import MongoClient\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(path):\n",
    "  g = gzip.open(path, 'rb')\n",
    "  for l in g:\n",
    "    yield json.loads(l)\n",
    "def getDF(path):\n",
    "  i = 0\n",
    "  df = {}\n",
    "  for d in parse(path):\n",
    "    df[i] = d\n",
    "    i += 1\n",
    "  return pd.DataFrame.from_dict(df, orient='index')\n",
    "purchasing_df = getDF('Data/Video_Games_5.json.gz')\n",
    "meta_datadf = getDF('Data/meta_Video_Games.json.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['vote', 'style', 'image'] \n",
    "purchasing_df = purchasing_df.drop(columns=columns_to_drop)\n",
    "positive_samples = purchasing_df.copy()\n",
    "positive_samples['label'] = 1 \n",
    "positive_samples['reviewTime'] = pd.to_datetime(positive_samples['reviewTime'])\n",
    "positive_samples.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_datadf = meta_datadf.drop_duplicates(subset='asin', keep='first')\n",
    "meta_datadf.reset_index(drop=True, inplace=True)\n",
    "meta_datadf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_samples = positive_samples.merge(meta_datadf, on='asin', how='inner')\n",
    "positive_samples['price'] = pd.to_numeric(positive_samples['price'].str.replace(r'[^\\d.]', '', regex=True), errors='coerce')\n",
    "positive_samples['price'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_to_instances = {} \n",
    "for index, row in positive_samples.iterrows(): \n",
    "    if row['reviewerID'] not in users_to_instances:\n",
    "        users_to_instances[row['reviewerID']] = 0\n",
    "    users_to_instances[row['reviewerID']] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_samples = positive_samples[positive_samples['reviewerID'].map(users_to_instances) >= 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_to_instances = {} \n",
    "for index, row in positive_samples.iterrows(): \n",
    "    if row['asin'] not in item_to_instances:\n",
    "        item_to_instances[row['asin']] = 0\n",
    "    item_to_instances[row['asin']] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_samples = positive_samples[positive_samples['asin'].map(item_to_instances) >= 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['imageURL', 'imageURLHighRes', 'reviewerName', 'reviewText', \n",
    "                   'unixReviewTime', 'date']\n",
    "positive_samples = positive_samples.drop(columns=columns_to_drop, axis=1)\n",
    "positive_samples = positive_samples.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient('mongodb://localhost:27017/')\n",
    "db = client['video_games_db']\n",
    "price_collection = db['keepa']\n",
    "query = {\"df_NEW\": {\"$exists\": True}}\n",
    "document_count = price_collection.count_documents(query)\n",
    "projection = {\"df_NEW\": 1, \"asin\": 1, \"_id\": 0}\n",
    "documents = price_collection.find(query, projection)\n",
    "price_df = pd.DataFrame(documents) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_considered = set() \n",
    "for index, row in positive_samples.iterrows(): \n",
    "    items_considered.add(row[\"asin\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asin_to_price = {}\n",
    "for index, row in price_df.iterrows():\n",
    "    asin_to_price[row[\"asin\"]] = row[\"df_NEW\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asin_to_positive = {} \n",
    "for index, row in positive_samples.iterrows(): \n",
    "    asin_to_positive[row['asin']] = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asin_to_meta = {}\n",
    "for index, row in meta_datadf.iterrows(): \n",
    "    asin_to_meta[row[\"asin\"]] = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "person_to_purchases = {} \n",
    "for index, row in positive_samples.iterrows(): \n",
    "    if row['reviewerID'] not in person_to_purchases:\n",
    "        person_to_purchases[row['reviewerID']] = set()\n",
    "    person_to_purchases[row['reviewerID']].add(row['asin'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "person_to_not_purchased = {} \n",
    "all_asins = set()\n",
    "for index, row in positive_samples.iterrows(): \n",
    "    all_asins.add(row['asin'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in positive_samples.iterrows():\n",
    "    if row['reviewerID'] not in person_to_not_purchased:\n",
    "        person_to_not_purchased[row['reviewerID']] = all_asins.copy()\n",
    "    if row['asin'] in person_to_not_purchased[row['reviewerID']]:\n",
    "        person_to_not_purchased[row['reviewerID']].remove(row['asin'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_search_closest(arr, target, compare):\n",
    "    low, high = 0, len(arr) - 1\n",
    "    closest = None\n",
    "    closest_diff = float('inf')\n",
    "    while low <= high:\n",
    "        mid = (low + high) // 2\n",
    "        result = compare(arr[mid], target)\n",
    "        if result == 0:\n",
    "            return arr[mid]\n",
    "        elif result < 0:\n",
    "            low = mid + 1\n",
    "        else:\n",
    "            high = mid - 1\n",
    "        diff = abs(result)\n",
    "        if diff < closest_diff:\n",
    "            closest_diff = diff\n",
    "            closest = arr[mid]\n",
    "    return closest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "removed_rows = set()\n",
    "for index, row in tqdm(positive_samples.iterrows(), total=positive_samples.shape[0], desc=\"Processing rows\"):\n",
    "    asin = row[\"asin\"]\n",
    "    current_date = row[\"reviewTime\"]\n",
    "    if asin in asin_to_price:\n",
    "        keys_to_be_removed = []\n",
    "        for key in asin_to_price[asin]:\n",
    "            if np.isnan(asin_to_price[asin][key]):\n",
    "                keys_to_be_removed.append(key)\n",
    "        for key in keys_to_be_removed:\n",
    "            del asin_to_price[asin][key]\n",
    "        price_list = list(asin_to_price[asin].keys())\n",
    "        if len(price_list) == 0:\n",
    "            removed_rows.add(index)\n",
    "        else: \n",
    "            closest_date = binary_search_closest(price_list, current_date, lambda x, y: (pd.to_datetime(x) - y).total_seconds())\n",
    "            closest_price = asin_to_price[asin][closest_date]\n",
    "            positive_samples.at[index, \"price\"] = closest_price\n",
    "    else:\n",
    "        removed_rows.add(index)\n",
    "print(len(removed_rows))                                                                                                                                                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_samples = positive_samples.drop(removed_rows)\n",
    "positive_samples.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(positive_samples.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_samples = positive_samples.dropna(subset=['price'])\n",
    "positive_samples.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(positive_samples.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_samples = positive_samples.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hq/878shj3x61n3xw850_m1scv00000gn/T/ipykernel_80809/1724587496.py:1: DtypeWarning: Columns (13,19) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  positive_samples = pd.read_csv('../samples/positive_samples.csv')\n"
     ]
    }
   ],
   "source": [
    "positive_samples = pd.read_csv('../samples/positive_samples.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_encoder = LabelEncoder()\n",
    "item_encoder = LabelEncoder()\n",
    "brand_encoder = LabelEncoder()\n",
    "positive_samples['user_id_encoded'] = user_encoder.fit_transform(positive_samples['reviewerID'])\n",
    "positive_samples['item_id_encoded'] = item_encoder.fit_transform(positive_samples['asin'])\n",
    "positive_samples['brand_id_encoded'] = brand_encoder.fit_transform(positive_samples['brand'])\n",
    "positive_samples = positive_samples[['user_id_encoded', 'item_id_encoded', 'brand_id_encoded', 'price', 'description', 'label']]\n",
    "num_users =  positive_samples[\"user_id_encoded\"].nunique()\n",
    "num_items = positive_samples[\"item_id_encoded\"].nunique()\n",
    "num_brands =  positive_samples[\"brand_id_encoded\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = positive_samples[['user_id_encoded', 'item_id_encoded', 'brand_id_encoded', 'price', 'description']].values\n",
    "y = positive_samples['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "positive_indices = np.where(y_train == 1)[0]\n",
    "X_train_positive = X_train[positive_indices]\n",
    "y_train_positive = y_train[positive_indices]\n",
    "positive_indices_test = np.where(y_test == 1)[0]\n",
    "X_test_positive = X_test[positive_indices_test]\n",
    "y_test_positive = y_test[positive_indices_test]\n",
    "\n",
    "X_train_user = X_train[:, 0].astype(np.int32)\n",
    "X_train_item = X_train[:,1].astype(np.int32)\n",
    "X_train_brand = X_train[:,2].astype(np.int32)\n",
    "X_train_price = X_train[:,3].astype(np.float32)\n",
    "X_train_description = X_train[:, 4].astype(np.str_)\n",
    "\n",
    "X_test_user = X_test[:, 0].astype(np.int32)\n",
    "X_test_item = X_test[:, 1].astype(np.int32)\n",
    "X_test_brand = X_test[:, 2].astype(np.int32)\n",
    "X_test_price = X_test[:, 3].astype(np.float32)\n",
    "X_test_description = X_test[:, 4].astype(np.str_)\n",
    "\n",
    "y_train = y_train.astype(np.float32)\n",
    "y_test = y_test.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "def yield_tokens(data_iter):\n",
    "    for text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "vocab = build_vocab_from_iterator(yield_tokens(X_train_description), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "def text_pipeline(text):\n",
    "    return [vocab[token] for token in tokenizer(text)]\n",
    "\n",
    "X_train_description_processed = [text_pipeline(text) for text in X_train_description]\n",
    "X_test_description_processed = [text_pipeline(text) for text in X_test_description]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = max(len(desc) for desc in X_train_description_processed + X_test_description_processed)\n",
    "\n",
    "def pad_sequence(seq):\n",
    "    return seq + [vocab[\"<unk>\"]] * (max_length - len(seq))\n",
    "\n",
    "X_train_description_padded = torch.tensor([pad_sequence(desc) for desc in X_train_description_processed], dtype=torch.long)\n",
    "X_test_description_padded = torch.tensor([pad_sequence(desc) for desc in X_test_description_processed], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserTower(nn.Module):\n",
    "    def __init__(self, user_dim, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(user_dim, embedding_dim)\n",
    "        self.fc = nn.Linear(embedding_dim, 64)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ItemTower(nn.Module):\n",
    "    def __init__(self, item_dim, brand_dim, vocab_size, embedding_dim, description_embedding_size):\n",
    "        super().__init__()\n",
    "        self.item_embedding = nn.Embedding(item_dim, embedding_dim)\n",
    "        self.brand_embedding = nn.Embedding(brand_dim, embedding_dim)\n",
    "        self.description_embedding = nn.Embedding(vocab_size, description_embedding_size)\n",
    "        self.fc = nn.Linear(embedding_dim * 2 + 1 + description_embedding_size, 64)\n",
    "        \n",
    "    def forward(self, item, brand, price, description):\n",
    "        item_emb = self.item_embedding(item)\n",
    "        brand_emb = self.brand_embedding(brand)\n",
    "        desc_vec = self.description_embedding(description).mean(dim=1)\n",
    "        x = torch.cat([item_emb, brand_emb, desc_vec, price.unsqueeze(1)], dim=1)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoTowerModel(nn.Module):\n",
    "    def __init__(self, user_dim, item_dim, brand_dim, vocab_size, embedding_dim, description_embedding_size):\n",
    "        super().__init__()\n",
    "        self.user_tower = UserTower(user_dim, embedding_dim)\n",
    "        self.item_tower = ItemTower(item_dim, brand_dim, vocab_size=vocab_size, embedding_dim = embedding_dim,description_embedding_size=description_embedding_size)\n",
    "        \n",
    "    def forward(self, user, item, brand, price, description):\n",
    "        user_emb = self.user_tower(user)\n",
    "        item_emb = self.item_tower(item, brand, price, description)\n",
    "        return (user_emb * item_emb).sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def in_batch_negative_sampling(user, item, brand, price, description):\n",
    "    batch_size = user.size(0)\n",
    "    users = user.repeat_interleave(batch_size)\n",
    "    items = item.repeat(batch_size)\n",
    "    brands = brand.repeat(batch_size)\n",
    "    prices = price.repeat(batch_size)\n",
    "    descriptions = description.repeat(batch_size, 1)\n",
    "    labels = torch.eye(batch_size).view(-1)\n",
    "    positive_indices = [i * (batch_size + 1) for i in range(batch_size)]\n",
    "    pos_users = users[positive_indices]\n",
    "    pos_items = items[positive_indices]\n",
    "    pos_brands = brands[positive_indices]\n",
    "    pos_prices = prices[positive_indices]\n",
    "    pos_description = descriptions[positive_indices]\n",
    "    pos_labels = labels[positive_indices]\n",
    "    neg_indices = []\n",
    "    for i in range(batch_size):\n",
    "        start = i * batch_size\n",
    "        end = (i + 1) * batch_size\n",
    "        possible_neg = list(range(start, end))\n",
    "        possible_neg.remove(start + i) \n",
    "        neg_indices.extend(random.sample(possible_neg, 1))\n",
    "    users = users[neg_indices]\n",
    "    items = items[neg_indices]\n",
    "    brands = brands[neg_indices]\n",
    "    prices = prices[neg_indices]\n",
    "    descriptions = descriptions[neg_indices]\n",
    "    labels = labels[neg_indices]\n",
    "    users = torch.cat([pos_users, users])\n",
    "    items = torch.cat([pos_items, items])\n",
    "    brands = torch.cat([pos_brands, brands])\n",
    "    prices = torch.cat([pos_prices, prices])\n",
    "    descriptions = torch.cat([pos_description, descriptions])\n",
    "    labels = torch.cat([pos_labels, labels])\n",
    "    return users, items, brands, prices, descriptions, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(outputs, labels):\n",
    "    predictions = (torch.sigmoid(outputs) > 0.5).float()\n",
    "    correct = (predictions == labels).float().sum()\n",
    "    return correct / len(labels)\n",
    "\n",
    "def train_model(model, train_loader, test_loader, optimizer, epochs):\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        total_accuracy = 0\n",
    "        num_batches = 0\n",
    "        all_outputs = []\n",
    "        all_labels = []\n",
    "        for batch in train_loader:\n",
    "            user, item, brand, price, description, labels = batch\n",
    "            #print the shapes\n",
    "            users, items, brands, prices, description, labels = in_batch_negative_sampling(user, item, brand, price, description)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(users, items, brands, prices, description)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total_accuracy += calculate_accuracy(outputs, labels)\n",
    "            num_batches += 1\n",
    "            all_outputs.extend(outputs.detach().cpu().numpy())\n",
    "            all_labels.extend(labels.detach().cpu().numpy())\n",
    "            \n",
    "        train_auc = roc_auc_score(all_labels, all_outputs)\n",
    "        train_loss = total_loss / num_batches\n",
    "        train_accuracy = total_accuracy / num_batches\n",
    "\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        total_val_accuracy = 0\n",
    "        num_val_batches = 0\n",
    "        all_val_outputs = []\n",
    "        all_val_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                user, item, brand, price, description, _ = batch\n",
    "                users, items, brands, prices, description, labels = in_batch_negative_sampling(user, item, brand, price, description)\n",
    "                outputs = model(users, items, brands, prices, description)\n",
    "                val_loss = criterion(outputs, labels)\n",
    "                total_val_loss += val_loss.item()\n",
    "                total_val_accuracy += calculate_accuracy(outputs, labels)\n",
    "                num_val_batches += 1\n",
    "                all_val_outputs.extend(outputs.detach().cpu().numpy())\n",
    "                all_val_labels.extend(labels.detach().cpu().numpy())\n",
    "        \n",
    "        val_loss = total_val_loss / num_val_batches\n",
    "        val_accuracy = total_val_accuracy / num_val_batches\n",
    "        \n",
    "        model.train()\n",
    "        val_auc = roc_auc_score(all_val_labels, all_val_outputs)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}\")\n",
    "        print(f\"Train AUC: {train_auc:.4f}\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
    "        print(f\"Val AUC: {val_auc:.4f}\")\n",
    "        print(\"--------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hq/878shj3x61n3xw850_m1scv00000gn/T/ipykernel_80809/1889991333.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_train_user = torch.tensor(X_train_user, dtype=torch.int32)\n",
      "/var/folders/hq/878shj3x61n3xw850_m1scv00000gn/T/ipykernel_80809/1889991333.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_train_item = torch.tensor(X_train_item, dtype=torch.int32)\n",
      "/var/folders/hq/878shj3x61n3xw850_m1scv00000gn/T/ipykernel_80809/1889991333.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_train_brand = torch.tensor(X_train_brand, dtype=torch.int32)\n",
      "/var/folders/hq/878shj3x61n3xw850_m1scv00000gn/T/ipykernel_80809/1889991333.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_train_price = torch.tensor(X_train_price, dtype=torch.float32)\n",
      "/var/folders/hq/878shj3x61n3xw850_m1scv00000gn/T/ipykernel_80809/1889991333.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_train = torch.tensor(y_train, dtype=torch.float32)\n",
      "/var/folders/hq/878shj3x61n3xw850_m1scv00000gn/T/ipykernel_80809/1889991333.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_test_user = torch.tensor(X_test_user, dtype=torch.int32)\n",
      "/var/folders/hq/878shj3x61n3xw850_m1scv00000gn/T/ipykernel_80809/1889991333.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_test_item = torch.tensor(X_test_item, dtype=torch.int32)\n",
      "/var/folders/hq/878shj3x61n3xw850_m1scv00000gn/T/ipykernel_80809/1889991333.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_test_brand = torch.tensor(X_test_brand, dtype=torch.int32)\n",
      "/var/folders/hq/878shj3x61n3xw850_m1scv00000gn/T/ipykernel_80809/1889991333.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_test_price = torch.tensor(X_test_price, dtype=torch.float32)\n",
      "/var/folders/hq/878shj3x61n3xw850_m1scv00000gn/T/ipykernel_80809/1889991333.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test = torch.tensor(y_test, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "Train Loss: 0.9055, Train Accuracy: 0.5017\n",
      "Train AUC: 0.5019\n",
      "Val Loss: 0.7196, Val Accuracy: 0.5018\n",
      "Val AUC: 0.5020\n",
      "--------------------\n",
      "Epoch 2/50\n",
      "Train Loss: 0.7052, Train Accuracy: 0.5129\n",
      "Train AUC: 0.5184\n",
      "Val Loss: 0.7037, Val Accuracy: 0.5065\n",
      "Val AUC: 0.5098\n",
      "--------------------\n",
      "Epoch 3/50\n",
      "Train Loss: 0.7003, Train Accuracy: 0.5253\n",
      "Train AUC: 0.5361\n",
      "Val Loss: 0.7078, Val Accuracy: 0.5171\n",
      "Val AUC: 0.5257\n",
      "--------------------\n",
      "Epoch 4/50\n",
      "Train Loss: 0.7025, Train Accuracy: 0.5496\n",
      "Train AUC: 0.5717\n",
      "Val Loss: 0.7074, Val Accuracy: 0.5443\n",
      "Val AUC: 0.5644\n",
      "--------------------\n",
      "Epoch 5/50\n",
      "Train Loss: 0.6872, Train Accuracy: 0.5869\n",
      "Train AUC: 0.6231\n",
      "Val Loss: 0.6988, Val Accuracy: 0.5710\n",
      "Val AUC: 0.6049\n",
      "--------------------\n",
      "Epoch 6/50\n",
      "Train Loss: 0.6693, Train Accuracy: 0.6244\n",
      "Train AUC: 0.6740\n",
      "Val Loss: 0.7040, Val Accuracy: 0.5959\n",
      "Val AUC: 0.6375\n",
      "--------------------\n",
      "Epoch 7/50\n",
      "Train Loss: 0.6265, Train Accuracy: 0.6711\n",
      "Train AUC: 0.7311\n",
      "Val Loss: 0.7147, Val Accuracy: 0.6247\n",
      "Val AUC: 0.6699\n",
      "--------------------\n",
      "Epoch 8/50\n",
      "Train Loss: 0.6036, Train Accuracy: 0.7067\n",
      "Train AUC: 0.7713\n",
      "Val Loss: 0.6790, Val Accuracy: 0.6469\n",
      "Val AUC: 0.7059\n",
      "--------------------\n",
      "Epoch 9/50\n",
      "Train Loss: 0.5544, Train Accuracy: 0.7406\n",
      "Train AUC: 0.8111\n",
      "Val Loss: 0.7072, Val Accuracy: 0.6706\n",
      "Val AUC: 0.7244\n",
      "--------------------\n",
      "Epoch 10/50\n",
      "Train Loss: 0.5216, Train Accuracy: 0.7643\n",
      "Train AUC: 0.8369\n",
      "Val Loss: 0.7032, Val Accuracy: 0.6776\n",
      "Val AUC: 0.7461\n",
      "--------------------\n",
      "Epoch 11/50\n",
      "Train Loss: 0.4782, Train Accuracy: 0.7885\n",
      "Train AUC: 0.8613\n",
      "Val Loss: 0.7155, Val Accuracy: 0.6889\n",
      "Val AUC: 0.7538\n",
      "--------------------\n",
      "Epoch 12/50\n",
      "Train Loss: 0.4610, Train Accuracy: 0.8004\n",
      "Train AUC: 0.8737\n",
      "Val Loss: 0.7089, Val Accuracy: 0.6967\n",
      "Val AUC: 0.7698\n",
      "--------------------\n",
      "Epoch 13/50\n",
      "Train Loss: 0.4702, Train Accuracy: 0.8083\n",
      "Train AUC: 0.8787\n",
      "Val Loss: 1.0665, Val Accuracy: 0.6827\n",
      "Val AUC: 0.7427\n",
      "--------------------\n",
      "Epoch 14/50\n",
      "Train Loss: 0.4469, Train Accuracy: 0.8196\n",
      "Train AUC: 0.8898\n",
      "Val Loss: 0.7384, Val Accuracy: 0.6993\n",
      "Val AUC: 0.7763\n",
      "--------------------\n",
      "Epoch 15/50\n",
      "Train Loss: 0.3956, Train Accuracy: 0.8396\n",
      "Train AUC: 0.9070\n",
      "Val Loss: 0.7864, Val Accuracy: 0.7078\n",
      "Val AUC: 0.7839\n",
      "--------------------\n",
      "Epoch 16/50\n",
      "Train Loss: 0.3860, Train Accuracy: 0.8443\n",
      "Train AUC: 0.9114\n",
      "Val Loss: 0.7852, Val Accuracy: 0.7065\n",
      "Val AUC: 0.7889\n",
      "--------------------\n",
      "Epoch 17/50\n",
      "Train Loss: 0.3778, Train Accuracy: 0.8512\n",
      "Train AUC: 0.9168\n",
      "Val Loss: 0.8508, Val Accuracy: 0.7100\n",
      "Val AUC: 0.7879\n",
      "--------------------\n",
      "Epoch 18/50\n",
      "Train Loss: 0.3456, Train Accuracy: 0.8627\n",
      "Train AUC: 0.9269\n",
      "Val Loss: 0.9048, Val Accuracy: 0.6956\n",
      "Val AUC: 0.7892\n",
      "--------------------\n",
      "Epoch 19/50\n",
      "Train Loss: 0.3462, Train Accuracy: 0.8659\n",
      "Train AUC: 0.9287\n",
      "Val Loss: 0.8848, Val Accuracy: 0.7074\n",
      "Val AUC: 0.7893\n",
      "--------------------\n",
      "Epoch 20/50\n",
      "Train Loss: 0.3500, Train Accuracy: 0.8681\n",
      "Train AUC: 0.9297\n",
      "Val Loss: 0.9692, Val Accuracy: 0.7070\n",
      "Val AUC: 0.7885\n",
      "--------------------\n",
      "Epoch 21/50\n",
      "Train Loss: 0.3490, Train Accuracy: 0.8693\n",
      "Train AUC: 0.9311\n",
      "Val Loss: 0.9682, Val Accuracy: 0.7013\n",
      "Val AUC: 0.7921\n",
      "--------------------\n",
      "Epoch 22/50\n",
      "Train Loss: 0.3310, Train Accuracy: 0.8755\n",
      "Train AUC: 0.9358\n",
      "Val Loss: 1.0390, Val Accuracy: 0.6979\n",
      "Val AUC: 0.7932\n",
      "--------------------\n",
      "Epoch 23/50\n",
      "Train Loss: 0.3604, Train Accuracy: 0.8725\n",
      "Train AUC: 0.9332\n",
      "Val Loss: 1.1648, Val Accuracy: 0.6969\n",
      "Val AUC: 0.7866\n",
      "--------------------\n",
      "Epoch 24/50\n",
      "Train Loss: 0.3238, Train Accuracy: 0.8805\n",
      "Train AUC: 0.9397\n",
      "Val Loss: 0.9365, Val Accuracy: 0.7185\n",
      "Val AUC: 0.8039\n",
      "--------------------\n",
      "Epoch 25/50\n",
      "Train Loss: 0.2978, Train Accuracy: 0.8896\n",
      "Train AUC: 0.9452\n",
      "Val Loss: 1.0136, Val Accuracy: 0.7143\n",
      "Val AUC: 0.8019\n",
      "--------------------\n",
      "Epoch 26/50\n",
      "Train Loss: 0.3235, Train Accuracy: 0.8840\n",
      "Train AUC: 0.9422\n",
      "Val Loss: 1.0797, Val Accuracy: 0.7010\n",
      "Val AUC: 0.7999\n",
      "--------------------\n",
      "Epoch 27/50\n",
      "Train Loss: 0.2912, Train Accuracy: 0.8940\n",
      "Train AUC: 0.9485\n",
      "Val Loss: 1.0961, Val Accuracy: 0.7087\n",
      "Val AUC: 0.8007\n",
      "--------------------\n",
      "Epoch 28/50\n",
      "Train Loss: 0.2843, Train Accuracy: 0.8975\n",
      "Train AUC: 0.9507\n",
      "Val Loss: 1.3521, Val Accuracy: 0.6891\n",
      "Val AUC: 0.7883\n",
      "--------------------\n",
      "Epoch 29/50\n",
      "Train Loss: 0.3385, Train Accuracy: 0.8866\n",
      "Train AUC: 0.9435\n",
      "Val Loss: 1.2653, Val Accuracy: 0.6983\n",
      "Val AUC: 0.7974\n",
      "--------------------\n",
      "Epoch 30/50\n",
      "Train Loss: 0.2872, Train Accuracy: 0.8954\n",
      "Train AUC: 0.9504\n",
      "Val Loss: 1.2308, Val Accuracy: 0.6955\n",
      "Val AUC: 0.7979\n",
      "--------------------\n",
      "Epoch 31/50\n",
      "Train Loss: 0.2869, Train Accuracy: 0.8986\n",
      "Train AUC: 0.9518\n",
      "Val Loss: 1.3132, Val Accuracy: 0.6940\n",
      "Val AUC: 0.7989\n",
      "--------------------\n",
      "Epoch 32/50\n",
      "Train Loss: 0.2779, Train Accuracy: 0.9018\n",
      "Train AUC: 0.9546\n",
      "Val Loss: 1.1768, Val Accuracy: 0.7080\n",
      "Val AUC: 0.8060\n",
      "--------------------\n",
      "Epoch 33/50\n",
      "Train Loss: 0.2648, Train Accuracy: 0.9060\n",
      "Train AUC: 0.9570\n",
      "Val Loss: 1.2731, Val Accuracy: 0.7013\n",
      "Val AUC: 0.7999\n",
      "--------------------\n",
      "Epoch 34/50\n",
      "Train Loss: 0.3388, Train Accuracy: 0.8936\n",
      "Train AUC: 0.9483\n",
      "Val Loss: 1.4926, Val Accuracy: 0.6878\n",
      "Val AUC: 0.7929\n",
      "--------------------\n",
      "Epoch 35/50\n",
      "Train Loss: 0.2866, Train Accuracy: 0.9012\n",
      "Train AUC: 0.9545\n",
      "Val Loss: 1.2220, Val Accuracy: 0.7059\n",
      "Val AUC: 0.8049\n",
      "--------------------\n",
      "Epoch 36/50\n",
      "Train Loss: 0.2569, Train Accuracy: 0.9102\n",
      "Train AUC: 0.9593\n",
      "Val Loss: 1.2436, Val Accuracy: 0.7058\n",
      "Val AUC: 0.8028\n",
      "--------------------\n",
      "Epoch 37/50\n",
      "Train Loss: 0.2562, Train Accuracy: 0.9119\n",
      "Train AUC: 0.9602\n",
      "Val Loss: 1.3556, Val Accuracy: 0.6996\n",
      "Val AUC: 0.7958\n",
      "--------------------\n",
      "Epoch 38/50\n",
      "Train Loss: 0.2981, Train Accuracy: 0.9023\n",
      "Train AUC: 0.9548\n",
      "Val Loss: 1.5055, Val Accuracy: 0.6860\n",
      "Val AUC: 0.7968\n",
      "--------------------\n",
      "Epoch 39/50\n",
      "Train Loss: 0.2844, Train Accuracy: 0.9056\n",
      "Train AUC: 0.9571\n",
      "Val Loss: 1.3375, Val Accuracy: 0.7008\n",
      "Val AUC: 0.8018\n",
      "--------------------\n",
      "Epoch 40/50\n",
      "Train Loss: 0.2622, Train Accuracy: 0.9104\n",
      "Train AUC: 0.9597\n",
      "Val Loss: 1.3827, Val Accuracy: 0.7010\n",
      "Val AUC: 0.8032\n",
      "--------------------\n",
      "Epoch 41/50\n",
      "Train Loss: 0.2435, Train Accuracy: 0.9157\n",
      "Train AUC: 0.9635\n",
      "Val Loss: 1.4099, Val Accuracy: 0.7067\n",
      "Val AUC: 0.8036\n",
      "--------------------\n",
      "Epoch 42/50\n",
      "Train Loss: 0.2442, Train Accuracy: 0.9164\n",
      "Train AUC: 0.9639\n",
      "Val Loss: 1.4622, Val Accuracy: 0.6967\n",
      "Val AUC: 0.8023\n",
      "--------------------\n",
      "Epoch 43/50\n",
      "Train Loss: 0.2743, Train Accuracy: 0.9110\n",
      "Train AUC: 0.9598\n",
      "Val Loss: 1.5449, Val Accuracy: 0.6874\n",
      "Val AUC: 0.7958\n",
      "--------------------\n",
      "Epoch 44/50\n",
      "Train Loss: 0.2745, Train Accuracy: 0.9105\n",
      "Train AUC: 0.9601\n",
      "Val Loss: 1.6966, Val Accuracy: 0.6880\n",
      "Val AUC: 0.7955\n",
      "--------------------\n",
      "Epoch 45/50\n",
      "Train Loss: 0.3061, Train Accuracy: 0.9045\n",
      "Train AUC: 0.9571\n",
      "Val Loss: 1.5028, Val Accuracy: 0.6959\n",
      "Val AUC: 0.8034\n",
      "--------------------\n",
      "Epoch 46/50\n",
      "Train Loss: 0.2598, Train Accuracy: 0.9142\n",
      "Train AUC: 0.9620\n",
      "Val Loss: 1.6513, Val Accuracy: 0.6948\n",
      "Val AUC: 0.8002\n",
      "--------------------\n",
      "Epoch 47/50\n",
      "Train Loss: 0.2458, Train Accuracy: 0.9178\n",
      "Train AUC: 0.9646\n",
      "Val Loss: 1.5864, Val Accuracy: 0.6877\n",
      "Val AUC: 0.7968\n",
      "--------------------\n",
      "Epoch 48/50\n",
      "Train Loss: 0.2612, Train Accuracy: 0.9175\n",
      "Train AUC: 0.9634\n",
      "Val Loss: 1.8263, Val Accuracy: 0.6944\n",
      "Val AUC: 0.7928\n",
      "--------------------\n",
      "Epoch 49/50\n",
      "Train Loss: 0.2512, Train Accuracy: 0.9188\n",
      "Train AUC: 0.9652\n",
      "Val Loss: 1.4380, Val Accuracy: 0.7020\n",
      "Val AUC: 0.8041\n",
      "--------------------\n",
      "Epoch 50/50\n",
      "Train Loss: 0.2632, Train Accuracy: 0.9173\n",
      "Train AUC: 0.9634\n",
      "Val Loss: 1.7551, Val Accuracy: 0.7031\n",
      "Val AUC: 0.7993\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "X_train_user = torch.tensor(X_train_user, dtype=torch.int32)\n",
    "X_train_item = torch.tensor(X_train_item, dtype=torch.int32)\n",
    "X_train_brand = torch.tensor(X_train_brand, dtype=torch.int32)\n",
    "X_train_price = torch.tensor(X_train_price, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "\n",
    "X_test_user = torch.tensor(X_test_user, dtype=torch.int32)\n",
    "X_test_item = torch.tensor(X_test_item, dtype=torch.int32)\n",
    "X_test_brand = torch.tensor(X_test_brand, dtype=torch.int32)\n",
    "X_test_price = torch.tensor(X_test_price, dtype=torch.float32)\n",
    "\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "\n",
    "train_dataset = TensorDataset(X_train_user, X_train_item, X_train_brand, X_train_price, X_train_description_padded, y_train)\n",
    "test_dataset = TensorDataset(X_test_user, X_test_item, X_test_brand, X_test_price, X_test_description_padded, y_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False)\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "model = TwoTowerModel(user_dim=num_users, item_dim=num_items, brand_dim=num_brands, vocab_size=vocab_size, embedding_dim=32, description_embedding_size=32)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "train_model(model, train_loader, test_loader, optimizer, epochs=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
